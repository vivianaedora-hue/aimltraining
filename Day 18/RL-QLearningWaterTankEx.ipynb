{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44f98ec-90d6-41bb-8a95-31b7a763eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd9fc1f-570e-4333-a6be-a72bbd46593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: [  0  10  20  30  40  50  60  70  80  90 100]\n",
      "Actions: ['FILL', 'STOP']\n"
     ]
    }
   ],
   "source": [
    "# Define State and Actions\n",
    "\n",
    "states = np.arange(0, 101, 10)\n",
    "# 0, 10, 20, 30, 40, ... 100\n",
    "\n",
    "actions = ['FILL', 'STOP']  # Fill or Stop\n",
    "\n",
    "print('States:', states)\n",
    "print('Actions:', actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339cb246-d096-4daa-b8b8-bffe79b11eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table shape: (11, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create Q-table and set alpha, gamma, epsilon, episodes\n",
    "\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "alpha = 0.1   # Learning rate\n",
    "gamma = 0.9   # Discount factor\n",
    "epsilon = 0.2 # Exploration rate\n",
    "episodes = 300 # Number of training runs\n",
    "\n",
    "print('Q-table shape:', Q.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bd06e1-9ab1-4f13-8e34-88dd46e5b323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Example (level=60, FILL): 10\n"
     ]
    }
   ],
   "source": [
    "def get_reward(level, action):\n",
    "    if 40 < level <= 70:\n",
    "        reward = 10  # ideal range\n",
    "    else:\n",
    "        reward = -10  # too low or too high\n",
    "\n",
    "    if action == 'FILL' and level >= 90:\n",
    "        reward -= 10  # overflow risk\n",
    "\n",
    "    if action == 'STOP' and level <= 10:\n",
    "        reward -= 10  # empty risk\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "print(\"Reward Example (level=60, FILL):\", get_reward(60, 'FILL'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b5279ba-a5cf-48a5-8a74-5a0a362d3792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Level Example: 55\n"
     ]
    }
   ],
   "source": [
    "def next_level(level, action):\n",
    "    if action == 'FILL':\n",
    "        level += random.choice([5, 10, 15])\n",
    "    else:\n",
    "        level -= random.choice([5, 10, 15])\n",
    "    \n",
    "    # Keep level within 0–100 range\n",
    "    return int(np.clip(level, 0, 100))\n",
    "\n",
    "print(\"Next Level Example:\", next_level(50, 'FILL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e616a6a-d5a2-42cc-8e22-ef9989bedbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b619a3fa-5ba1-4796-a815-c71991419a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "for ep in range(episodes):\n",
    "    level = random.choice(states)\n",
    "\n",
    "    for _ in range(15):  # steps per episode\n",
    "        # Choose action (epsilon-greedy)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = actions[np.argmax(Q[level // 10])]  # use integer index for level\n",
    "\n",
    "        # Get next state and reward\n",
    "        next_state = next_level(level, action)\n",
    "        reward = get_reward(next_state, action)\n",
    "\n",
    "        # Q-learning update\n",
    "        a = actions.index(action)\n",
    "        best_next = np.max(Q[next_state // 10])  # best action for next state\n",
    "        Q[level // 10, a] += alpha * (reward + gamma * best_next - Q[level // 10, a])\n",
    "\n",
    "        # Move to next state\n",
    "        level = next_state\n",
    "\n",
    "print('Training Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f4c568c-6aef-44d1-9a21-270b801b377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter starting water level (0-100):  50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting with level: 50%\n",
      "Simulating for 10 steps:\n",
      "\n",
      "Step 1: Level 50% → Action: FILL\n",
      "Step 2: Level 55% → Action: FILL\n",
      "Step 3: Level 65% → Action: STOP\n",
      "Step 4: Level 50% → Action: FILL\n",
      "Step 5: Level 60% → Action: STOP\n",
      "Step 6: Level 45% → Action: FILL\n",
      "Step 7: Level 55% → Action: FILL\n",
      "Step 8: Level 65% → Action: STOP\n",
      "Step 9: Level 50% → Action: FILL\n",
      "Step 10: Level 55% → Action: FILL\n",
      "\n",
      "Simulation complete. Water tank control finished.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    level = int(input('Enter starting water level (0-100): '))\n",
    "    if level < 0 or level > 100:\n",
    "        raise ValueError('Water level out of range!')\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    level = 50\n",
    "    print('Defaulting to 50%.')\n",
    "\n",
    "print(f'\\nStarting with level: {level}%')\n",
    "print(\"Simulating for 10 steps:\\n\")\n",
    "\n",
    "for step in range(10):\n",
    "    action = actions[np.argmax(Q[level // 10])]\n",
    "    print(f\"Step {step + 1}: Level {level}% → Action: {action}\")\n",
    "    level = next_level(level, action)\n",
    "\n",
    "print(\"\\nSimulation complete. Water tank control finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4dc089-45b6-411b-a72b-1b8f47f2af0c",
   "metadata": {},
   "source": [
    "## Exercise: Smart Traffic Light Controller using Q-learning\n",
    "\n",
    "    ## Objective: In this exercise, you'll design a Smart Traffic Light System that learns when to switch lights (Green/Red) based on real-time traffic consitions using Q-Learning. Your AI Agent will balance reducing waiting time for vehicles, saving energy, and maintaining safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa4a1f4-10e4-4871-8957-9d9f0efc7ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries (NumPy, random) imported successfully.\n"
     ]
    }
   ],
   "source": [
    "#Step 1 \n",
    "import numpy as np\n",
    "import random\n",
    "print(\"Required libraries (NumPy, random) imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feecfb1b-9e8a-492d-9f46-65fac709cc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defined Traffic States (0-4):\n",
      "  0: Empty Road\n",
      "  1: Light Traffic\n",
      "  2: Moderate Traffic\n",
      "  3: Heavy Traffic\n",
      "  4: Very Heavy Traffic\n",
      "\n",
      "Defined Actions (0-1):\n",
      "  0: GREEN (Keep Green)\n",
      "  1: RED (Turn Red)\n"
     ]
    }
   ],
   "source": [
    "# Define State and Actions\n",
    "# States: Traffic levels (0 = Empty, 4 = Very Heavy)\n",
    "states = {\n",
    "    0: \"Empty Road\",\n",
    "    1: \"Light Traffic\",\n",
    "    2: \"Moderate Traffic\",\n",
    "    3: \"Heavy Traffic\",\n",
    "    4: \"Very Heavy Traffic\"\n",
    "}\n",
    "num_states = len(states)\n",
    "\n",
    "# Actions: Decisions for the traffic light\n",
    "# These indices correspond to the columns in the Q-table.\n",
    "actions = {\n",
    "    0: \"GREEN (Keep Green)\",\n",
    "    1: \"RED (Turn Red)\"\n",
    "}\n",
    "action_names = list(actions.values())\n",
    "num_actions = len(actions)\n",
    "\n",
    "print(\"\\nDefined Traffic States (0-4):\")\n",
    "for k, v in states.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nDefined Actions (0-1):\")\n",
    "for k, v in actions.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a47ca8ca-ce70-4e89-90e7-aec867953d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table shape: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize the Q-Table and Hyperparameters\n",
    "Q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  \n",
    "gamma = 0.9  \n",
    "epsilon = 0.2  \n",
    "episodes = 300 \n",
    "\n",
    "print('Q-Table shape:', Q_table.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea9b9646-bbfe-43d4-9694-d2b7dda3f663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Example (level=0, RED): -5\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Design the Reward Function \n",
    "def get_reward(traffic_level, action):\n",
    "    GREEN = 0\n",
    "    RED = 1\n",
    "\n",
    "    # Heavy/Very Heavy Traffic (3, 4)\n",
    "    if traffic_level >= 3:\n",
    "        if action == GREEN:\n",
    "            return 10  # Good: clears congestion\n",
    "        else:\n",
    "            return -10 # Bad: creates jams\n",
    "\n",
    "    # Empty Road (0)\n",
    "    elif traffic_level == 0:\n",
    "        if action == RED:\n",
    "            return 5   # Good: Saves energy\n",
    "        else:\n",
    "            return -5  # Bad: Wastes power\n",
    "\n",
    "    # Light/Moderate Traffic (1, 2)\n",
    "    else:\n",
    "        return 1\n",
    "print(\"Reward Example (level=0, RED):\", get_reward(0, 'RED'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0500da1-bb35-497e-bc17-42022aa46770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define Environment Dynamics\n",
    "def next_traffic(current_traffic):\n",
    "   \n",
    "    change = random.choice([-1, 0, 1])\n",
    "    new_traffic = current_traffic + change\n",
    "    # Use np.clip to ensure the traffic level stays between 0 and 4\n",
    "    return np.clip(new_traffic, 0, 4)\n",
    "\n",
    "def choose_action(state, Q_table, epsilon):\n",
    "   \n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(list(actions.keys()))\n",
    "    else:\n",
    "        return np.argmax(Q_table[state, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c48caaec-27d8-4e73-bad0-8a38e0e3781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "#  Step 6: Train the Agent \n",
    "def train_agent(Q_table, episodes, alpha, gamma, epsilon):\n",
    "    print(\" Starting Q-Learning Training \")\n",
    "    for episode in range(episodes):\n",
    "       \n",
    "        current_state = random.randint(0, num_states - 1)\n",
    "        action = choose_action(current_state, Q_table, epsilon)\n",
    "        \n",
    "        reward = get_reward(current_state, action)\n",
    "        next_state = next_traffic(current_state)\n",
    "\n",
    "        max_future_q = np.max(Q_table[next_state, :])\n",
    "        Q_table[current_state, action] += alpha * reward + gamma * max_future_q - Q_table[current_state, action]\n",
    "\n",
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5ef0cac-79a5-47ba-a3d2-644a3c294e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter traffic level (0-4):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting with level: 1\n",
      "Simulating traffic control for 10 steps:\n",
      "\n",
      "Step 1: Level 1 → Action: GREEN (Keep Green)\n",
      "Step 2: Level 1 → Action: GREEN (Keep Green)\n",
      "Step 3: Level 2 → Action: GREEN (Keep Green)\n",
      "Step 4: Level 1 → Action: GREEN (Keep Green)\n",
      "Step 5: Level 0 → Action: GREEN (Keep Green)\n",
      "Step 6: Level 0 → Action: GREEN (Keep Green)\n",
      "Step 7: Level 1 → Action: GREEN (Keep Green)\n",
      "Step 8: Level 2 → Action: GREEN (Keep Green)\n",
      "Step 9: Level 1 → Action: GREEN (Keep Green)\n",
      "Step 10: Level 1 → Action: GREEN (Keep Green)\n",
      "\n",
      "Simulation complete. Traffic control finished.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Test traffic Controller\n",
    "\n",
    "try:\n",
    "    level = int(input('Enter traffic level (0-4): '))\n",
    "    if level < 0 or level > 4:\n",
    "        raise ValueError('Traffic level out of range!')\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    level = 4\n",
    "    print('Defaulting to 4.')\n",
    "\n",
    "print(f'\\nStarting with level: {level}')\n",
    "print(\"Simulating traffic control for 10 steps:\\n\")\n",
    "\n",
    "for step in range(10):\n",
    "   \n",
    "    action = actions[np.argmax(Q[level])]\n",
    "    print(f\"Step {step + 1}: Level {level} → Action: {action}\")\n",
    "    \n",
    "    level = next_traffic(level)\n",
    "\n",
    "print(\"\\nSimulation complete. Traffic control finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53255ef3-4aea-4ef4-8c96-56bca145f140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
